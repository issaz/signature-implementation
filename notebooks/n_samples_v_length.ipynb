{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc6216",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import iisignature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sigkernel\n",
    "import torch\n",
    "import torchsde\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.utils.helper_functions.plot_helper_functions import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ae5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if is_cuda else 'cpu'\n",
    "\n",
    "if not is_cuda:\n",
    "    print(\"Warning: CUDA not available; falling back to CPU but this is likely to be very slow.\")\n",
    "    \n",
    "# You realistically need GPU access (either natively or via cloud computing) to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e770eb",
   "metadata": {},
   "source": [
    "## 1. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef344e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HestonModel(torch.nn.Module):\n",
    "    def __init__(self, mu, kappa, theta, sigma, rho):\n",
    "        super(HestonModel, self).__init__()\n",
    "        # Parameters as tensors\n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "        self.kappa = torch.tensor(kappa, dtype=torch.float32)\n",
    "        self.theta = torch.tensor(theta, dtype=torch.float32)\n",
    "        self.sigma = torch.tensor(sigma, dtype=torch.float32)\n",
    "        self.rho = torch.tensor(rho, dtype=torch.float32)\n",
    "        \n",
    "        # Specify the noise type as 'general'\n",
    "        self.noise_type = 'general'\n",
    "        self.sde_type = 'ito'\n",
    "\n",
    "    def f(self, t, y):\n",
    "        # Drift part\n",
    "        S, V = y[..., 0], y[..., 1]\n",
    "        dS = self.mu * S  # Change this\n",
    "        dV = self.kappa * (self.theta - V)  # Change this\n",
    "        return torch.stack([dS, dV], dim=-1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        # Diffusion part corrected to account for noise dimensionality\n",
    "        S, V = y[..., 0], y[..., 1]\n",
    "        vol_S = torch.sqrt(V)  # Change this\n",
    "        vol_v = self.sigma * torch.sqrt(V)  # Change this\n",
    "\n",
    "        # Constructing a tensor of shape (batch_size, state_dim, noise_dim)\n",
    "        dW1_dS = vol_S * S  # dW1 effect on S  # Change this\n",
    "        dW1_dV = torch.zeros_like(S)  # dW1 has no direct effect on V  # Change this\n",
    "        \n",
    "        dW2_dS = torch.zeros_like(S)  # dW2 has no direct effect on S, Change this\n",
    "        dW2_dV = self.rho * vol_S + torch.sqrt(1 - self.rho ** 2) * vol_v  # dW2 effect on V, Change this\n",
    "\n",
    "        # Stacking to get the correct shape: (batch, state_channels, noise_channels)\n",
    "        return torch.stack([torch.stack([dW1_dS, dW1_dV], dim=-1),\n",
    "                            torch.stack([dW2_dS, dW2_dV], dim=-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the Heston model\n",
    "mu    = 0.05\n",
    "kappa = 1.5\n",
    "theta = 0.04\n",
    "sigma = 0.2\n",
    "rho   = -0.7\n",
    "\n",
    "# Initial conditions\n",
    "S0 = 100  # Initial asset price\n",
    "v0 = 0.04  # Initial variance\n",
    "y0 = torch.tensor([[S0, v0]] * 100, dtype=torch.float32)\n",
    "\n",
    "# Simulation settings\n",
    "t0, T, dt = 0, 1, 0.01  # start time, end time, and time step\n",
    "ts = torch.arange(t0, T, dt)\n",
    "\n",
    "# Create an instance of the Heston model and simulate\n",
    "heston_model = HestonModel(mu, kappa, theta, sigma, rho)\n",
    "result = torchsde.sdeint(heston_model, y0, ts, dt=dt, method=\"euler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, can use Brownian Motions (scaled, with drift...)\n",
    "class BrownianMotionDrift(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, mu, sigma, noise_type: str, sde_type: str):\n",
    "        super().__init__()\n",
    "        self.mu         = torch.nn.Parameter(torch.tensor(mu, dtype=torch.float32), requires_grad=True) \n",
    "        self.sigma      = torch.nn.Parameter(torch.tensor(sigma, dtype=torch.float32), requires_grad=True)\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type   = sde_type\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        # Directly return a tensor filled with self.mu without creating a zeros tensor first\n",
    "        return torch.full_like(y, self.mu.item(), dtype=torch.float32)\n",
    "    \n",
    "    def g(self, t, y):\n",
    "        # Directly return a tensor filled with self.sigma without creating a zeros tensor first\n",
    "        return torch.full_like(y, self.sigma.item(), dtype=torch.float32)\n",
    "    \n",
    "\n",
    "def return_mmd_distributions(h0_paths, h1_paths, kernel, n_atoms=128, n_paths=32, max_batch=32):\n",
    "\n",
    "    h0_dists = torch.zeros(n_atoms)\n",
    "    h1_dists = torch.zeros(n_atoms)\n",
    "    \n",
    "    h0_path_bank_size = h0_paths.shape[0]\n",
    "    h1_path_bank_size = h1_paths.shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_atoms):\n",
    "            h0_rands = torch.randperm(h0_path_bank_size)[:int(2*n_paths)]\n",
    "\n",
    "            ix, jx = h0_rands[:n_paths], h0_rands[n_paths:]\n",
    "            iy     = torch.randperm(h1_path_bank_size)[:n_paths]\n",
    "            \n",
    "            h0_dists[i] = kernel.compute_mmd(h0_paths[ix], h0_paths[jx], max_batch=max_batch)\n",
    "            h1_dists[i] = kernel.compute_mmd(h0_paths[ix], h1_paths[iy], max_batch=max_batch)\n",
    "            \n",
    "    return h0_dists, h1_dists\n",
    "\n",
    "def expected_type2_error(dist, crit_value):\n",
    "    n_atoms = dist.shape[0]\n",
    "    num_fail = dist <= crit_value\n",
    "    return sum(num_fail)/n_atoms\n",
    "\n",
    "def scale_transform(path: torch.tensor, scaler: torch.float32) -> torch.tensor:\n",
    "    device = path.device\n",
    "    res    = torch.zeros(path.shape).to(device)\n",
    "    \n",
    "    scaler_ = torch.tensor(scaler).to(device)\n",
    "    \n",
    "    res[..., 1:] = path[..., 1:]*scaler_\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6443372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sde params\n",
    "mu0, sig  = 0., 0.2\n",
    "mu1, beta = 0., 0.3\n",
    "noise_type = \"diagonal\"\n",
    "sde_type   = \"ito\"\n",
    "\n",
    "# Grid params\n",
    "T          = 1\n",
    "batch_size = 32768\n",
    "state_size = 1\n",
    "dt_scale   = 1e-1  # Finer refinements give better solutions (but slower)\n",
    "\n",
    "h0_model = BrownianMotionDrift(mu0, sig, noise_type, sde_type).to(device)\n",
    "h1_model = BrownianMotionDrift(mu1, beta, noise_type, sde_type).to(device)\n",
    "y0 = torch.full(size=(batch_size, state_size), fill_value=0.).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths       = [32, 64, 128]\n",
    "n_grid_points = [32, 64, 128]\n",
    "n_atoms       = 1024\n",
    "\n",
    "dyadic_order     = 0\n",
    "static_kernel    = sigkernel.LinearKernel()\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=dyadic_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cce22",
   "metadata": {},
   "source": [
    "### 2. Generate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ad753",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_h0 = torch.zeros((len(n_paths), len(n_grid_points), n_atoms))\n",
    "mmd_h1 = torch.zeros((len(n_paths), len(n_grid_points), n_atoms))\n",
    "_scaler = 5\n",
    "\n",
    "for i, gp in enumerate(tqdm(n_grid_points)):\n",
    "    ts = torch.linspace(0, T, gp).to(device)\n",
    "\n",
    "    _dt = dt_scale*torch.diff(ts)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h0_paths = torchsde.sdeint(h0_model, y0, ts, method='euler', dt = _dt).to(device)\n",
    "        h1_paths = torchsde.sdeint(h1_model, y0, ts, method='euler', dt = _dt).to(device)\n",
    "        \n",
    "        h0_paths = torch.cat([\n",
    "            ts.unsqueeze(-1).expand(batch_size, ts.size(0), 1), \n",
    "            torch.transpose(h0_paths, 1, 0)\n",
    "        ], dim=2)\n",
    "        \n",
    "        h1_paths = torch.cat([\n",
    "            ts.unsqueeze(-1).expand(batch_size, ts.size(0), 1), \n",
    "            torch.transpose(h1_paths, 1, 0)\n",
    "        ], dim=2)\n",
    "        \n",
    "        t_h0_paths = scale_transform(h0_paths.clone(), _scaler)\n",
    "        t_h1_paths = scale_transform(h1_paths.clone(), _scaler)\n",
    "        t_h0_paths[..., 0] /= _scaler*T\n",
    "        t_h1_paths[..., 0] /= _scaler*T\n",
    "        \n",
    "    for j, np in enumerate(n_paths):\n",
    "        h0_dists, h1_dists = return_mmd_distributions(\n",
    "            t_h0_paths, \n",
    "            t_h1_paths, \n",
    "            signature_kernel, \n",
    "            n_atoms=n_atoms, \n",
    "            n_paths=np, \n",
    "            max_batch=32\n",
    "        )\n",
    "        \n",
    "        mmd_h0[i, j] = h0_dists\n",
    "        mmd_h1[i, j] = h1_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_  = len(n_paths)\n",
    "ngp_ = len(n_grid_points)\n",
    "\n",
    "fig, axes = plt.subplots(np_, ngp_, figsize=(6*ngp_, 6*np_))\n",
    "n_bins = int(n_atoms/16)\n",
    "\n",
    "for i in range(ngp_):\n",
    "    for j  in range(np_):\n",
    "        this_h0 = mmd_h0[i, j]\n",
    "        this_h1 = mmd_h1[i, j]\n",
    "        \n",
    "        crit_val = this_h0.sort()[0][int(n_atoms*(0.95))]\n",
    "\n",
    "        gp_ = n_grid_points[i]\n",
    "        pt_ = n_paths[j]\n",
    "        axes[i,j].set_title(f\"l = {gp_}, n = {pt_}. Expected Type II error: {100*expected_type2_error(this_h1, crit_val):.2f}%\", fontsize=\"small\")\n",
    "        axes[i,j].hist(sorted(this_h0), bins=n_bins, color=\"dodgerblue\", alpha=0.5, label=\"$H_0$\", density=True)\n",
    "        axes[i,j].hist(sorted(this_h1), bins=n_bins, color=\"tomato\"    , alpha=0.5, label=\"$H_1$\", density=True)\n",
    "        #plt.legend()\n",
    "        make_grid(axis=axes[i,j])\n",
    "        \n",
    "plt.savefig(\"type_2_worked_paths_length.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecdc23",
   "metadata": {},
   "source": [
    "## 3. Lead-lag plots for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ar3_path(n, phi1, phi2, phi3, sigma=1):\n",
    "    \"\"\"Generate an AR(3) process.\"\"\"\n",
    "    path = np.zeros(n)\n",
    "    eps = np.random.normal(0, sigma, n)\n",
    "    path[1:3] = eps[1:3]\n",
    "    \n",
    "    for t in range(3, n):\n",
    "        path[t] = phi1 * path[t-1] + phi2 * path[t-2] + phi3 * path[t-3] + eps[t]\n",
    "    return path\n",
    "\n",
    "# Parameters for each scenario\n",
    "n = 128\n",
    "phi1, phi2, phi3 = 0.3, 0.3, 0.05\n",
    "\n",
    "correlated_params      = (phi1, phi2, phi3)\n",
    "uncorrelated_params    = (phi1, -phi2, 0.00)\n",
    "anti_correlated_params = (-phi1, -phi2, -phi3)\n",
    "\n",
    "# Generate paths\n",
    "n_paths = 1024\n",
    "correlated_rets      = np.array([generate_ar3_path(n, *correlated_params) for _ in range(n_paths)])/100\n",
    "uncorrelated_rets    = np.array([generate_ar3_path(n, *uncorrelated_params) for _ in range(n_paths)])/100\n",
    "anti_correlated_rets = np.array([generate_ar3_path(n, *anti_correlated_params) for _ in range(n_paths)])/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f84a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into price processes\n",
    "price_processes = False\n",
    "if price_processes:\n",
    "    correlated_paths = (1 + correlated_rets).cumprod(axis=1)\n",
    "    uncorrelated_paths = (1 + uncorrelated_rets).cumprod(axis=1)\n",
    "    anti_correlated_paths = (1 + anti_correlated_rets).cumprod(axis=1)\n",
    "else:\n",
    "    correlated_paths = correlated_rets\n",
    "    uncorrelated_paths = uncorrelated_rets\n",
    "    anti_correlated_paths = anti_correlated_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, q, z in zip(correlated_paths[:16], uncorrelated_paths[:16], anti_correlated_paths[:16]):\n",
    "    plt.plot(p, alpha=0.25, color=\"dodgerblue\")\n",
    "    plt.plot(q, alpha=0.25, color=\"tomato\")\n",
    "    plt.plot(z, alpha=0.25, color=\"seagreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lead_lag_transform(bank: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "    n_paths, length, dim = bank.size()\n",
    "\n",
    "    res_length = 2 * length - 1\n",
    "    res_dim    = 2*dim\n",
    "\n",
    "    res = torch.zeros((n_paths, res_length, res_dim))\n",
    "\n",
    "    # Add lagged paths\n",
    "    for i in 2*np.arange(dim):\n",
    "        lagged_values   = torch.repeat_interleave(bank.clone(), repeats=2, dim=1)[..., int(i/2)]\n",
    "        res[..., i]     = lagged_values[:, :-1]\n",
    "        res[..., i + 1] = lagged_values[:, 1:]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_correlated      = lead_lag_transform(torch.tensor(correlated_paths).unsqueeze(-1))\n",
    "ll_uncorrelated    = lead_lag_transform(torch.tensor(uncorrelated_paths).unsqueeze(-1))\n",
    "ll_anti_correlated = lead_lag_transform(torch.tensor(anti_correlated_paths).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ll_correlated = ll_correlated.mean(axis=0)\n",
    "average_ll_uncorrelated = ll_uncorrelated.mean(axis=0)\n",
    "average_ll_anti_correlated = ll_anti_correlated.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9975030",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "ax1.plot(average_ll_correlated[..., 0], average_ll_correlated[..., 1], alpha=0.5, label=\"correlated\", color=\"dodgerblue\")\n",
    "ax2.plot(average_ll_uncorrelated[..., 0], average_ll_uncorrelated[..., 1], alpha=0.5, label=\"uncorrelated\", color=\"tomato\")\n",
    "ax3.plot(average_ll_anti_correlated[..., 0], average_ll_anti_correlated[..., 1], alpha=0.5, label=\"anti_correlated\", color=\"seagreen\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab19c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = iisignature.prepare (2, 2)\n",
    "logsig_correl = iisignature.logsig(ll_correlated.numpy(), s)\n",
    "logsig_uncorrel = iisignature.logsig(ll_uncorrelated.numpy(), s)\n",
    "logsig_anticorrel = iisignature.logsig(ll_anti_correlated.numpy(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.set_xlabel(\"$S^{[1, 2]}(\\mathsf{x})$\")\n",
    "ax.hist(-0.5*logsig_correl[:, -1], bins=64, alpha=0.5, color=\"dodgerblue\", density=True, label=\"correlated_returns\")\n",
    "ax.hist(-0.5*logsig_uncorrel[:, -1], bins=64, alpha=0.5, color=\"tomato\", density=True, label=\"uncorrelated_returns\")\n",
    "ax.hist(-0.5*logsig_anticorrel[:, -1], bins=64, alpha=0.5, color=\"seagreen\", density=True, label=\"anti_correlated_returns\")\n",
    "ax.legend(fontsize=\"small\")\n",
    "ax.set_title(\"Distribution of second-order log-signature terms\", fontsize=\"medium\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"second_order_logdist.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
