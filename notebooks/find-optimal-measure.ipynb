{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sigkernel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchsde\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.sdes import ScaledBrownianMotion\n",
    "from src.utils.helper_functions.plot_helper_functions import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d93683f92d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784f433",
   "metadata": {},
   "source": [
    "### 1. Train FFN to learn the map from parameters to $(1-\\alpha)\\%$ of Gamma dist\n",
    "\n",
    "This is to ensure we can backprop.\n",
    "\n",
    "Want to learn the map from $(\\mu, \\theta)$ to the $1-\\alpha$ critical threshold. We generate uniformly some values of $(\\mu, \\theta)$ and train the model to learn the associated $(1-\\alpha)$ quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    model = torch.load('entire_model.pth')\n",
    "    model.eval()  # Important to set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a848bc",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_ppf(q: float, alpha, beta):\n",
    "    \"\"\"\n",
    "    Calculates quantile function of corresponding distribution for the inverse of the CDF at the level q\n",
    "\n",
    "    :param q:   Quantile to calculate inverse cdf of\n",
    "    :return:    Value of quantile function at q\n",
    "    \"\"\"\n",
    "    return stats.gamma.ppf(q, a=alpha, scale=beta)\n",
    "\n",
    "\n",
    "n_train_samples = 8192*8\n",
    "n_val_samples   = 8192\n",
    "n_test_samples  = 8192\n",
    "alpha           = 0.05\n",
    "scale_data      = True\n",
    "\n",
    "mu_ub    = 1000\n",
    "theta_ub = 100\n",
    "\n",
    "batch_size      = 512\n",
    "# mu_gen          = lambda x: torch.empty(x, 1, dtype=torch.float).uniform_(0, mu_ub)\n",
    "# theta_gen       = lambda x: torch.empty(x, 1, dtype=torch.float).uniform_(0, theta_ub)\n",
    "mu_gen          = lambda x: torch.empty(x, 1, dtype=torch.float).exponential_(1/500)\n",
    "theta_gen       = lambda x: torch.empty(x, 1, dtype=torch.float).exponential_(1/50)\n",
    "\n",
    "train_mu     = mu_gen(n_train_samples)\n",
    "train_theta  = theta_gen(n_train_samples)\n",
    "train_labels = torch.tensor([gamma_ppf(1-alpha, mu, theta) for mu, theta in zip(train_mu, train_theta)])\n",
    "\n",
    "test_mu     = mu_gen(n_test_samples)\n",
    "test_theta  = theta_gen(n_test_samples)\n",
    "test_labels = torch.tensor([gamma_ppf(1-alpha, mu, theta) for mu, theta in zip(test_mu, test_theta)])\n",
    "\n",
    "val_mu     = mu_gen(n_val_samples)\n",
    "val_theta  = theta_gen(n_val_samples)\n",
    "val_labels = torch.tensor([gamma_ppf(1-alpha, mu, theta) for mu, theta in zip(val_mu, val_theta)])\n",
    "\n",
    "train_X = torch.cat([train_mu, train_theta], axis=1).to(device).float()\n",
    "test_X  = torch.cat([test_mu, test_theta], axis=1).to(device).float()\n",
    "val_X  = torch.cat([val_mu, val_theta], axis=1).to(device).float()\n",
    "\n",
    "train_Y = train_labels.to(device).float()\n",
    "test_Y  = test_labels.to(device).float()\n",
    "val_Y = val_labels.to(device).float()\n",
    "\n",
    "# Scale\n",
    "if scale_data:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train_X.cpu().numpy())\n",
    "    \n",
    "    train_X = torch.tensor(scaler.transform(train_X.cpu().numpy())).to(device).float()\n",
    "    test_X = torch.tensor(scaler.transform(test_X.cpu().numpy())).to(device).float() # use the same scaler on test data\n",
    "    val_X = torch.tensor(scaler.transform(val_X.cpu().numpy())).to(device).float() # and validation data\n",
    "\n",
    "train_dataset = TensorDataset(train_X, train_Y)\n",
    "train_loader  = DataLoader(dataset=train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27537917",
   "metadata": {},
   "source": [
    "#### Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1778af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, order=3):\n",
    "    \"\"\"Generate polynomial features up to a given order for tensor x.\"\"\"\n",
    "    x_poly = x.clone()\n",
    "    for i in range(2, order + 1):\n",
    "        x_poly = torch.cat((x_poly, x ** i), dim=1)\n",
    "    return x_poly\n",
    "\n",
    "# Polynomial Regression Model\n",
    "class PolynomialRegressionModel(nn.Module):\n",
    "    def __init__(self, order, dim):\n",
    "        super(PolynomialRegressionModel, self).__init__()\n",
    "        # Adjust the input feature size based on the polynomial order\n",
    "        self.order  = order\n",
    "        self.linear = nn.Linear(sum((dim**o for o in range(1, order))), 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = polynomial_features(x, self.order)\n",
    "        return self.linear(features)\n",
    "    \n",
    "def inverse_distance_weighting(x, y, z, x_new, y_new, power=2):\n",
    "    \"\"\"\n",
    "    Perform inverse distance weighting interpolation for scattered data.\n",
    "    \n",
    "    x, y: Coordinates of the data points.\n",
    "    z: Values at the data points.\n",
    "    x_new, y_new: Coordinates of the point to interpolate.\n",
    "    power: Power parameter for the weighting. Higher values assign greater influence to closer points.\n",
    "    \"\"\"\n",
    "    # Calculate squared distances from the new point to all existing points\n",
    "    distances_sq = (x - x_new) ** 2 + (y - y_new) ** 2\n",
    "    \n",
    "    # Avoid division by zero for the exact location points\n",
    "    distances_sq = torch.clamp(distances_sq, min=1e-6)\n",
    "    \n",
    "    # Calculate weights based on inverse distance\n",
    "    weights = 1 / distances_sq ** (power / 2)\n",
    "    \n",
    "    # Compute weighted average\n",
    "    z_new = torch.sum(weights * z) / torch.sum(weights)\n",
    "    \n",
    "    return z_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d280b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs            = 500\n",
    "batch_itr         = n_train_samples/batch_size\n",
    "losses            = torch.zeros(int(epochs*batch_itr))\n",
    "count             = 0\n",
    "patience          = 20  # how many epochs to wait for improvement in the validation loss before stopping\n",
    "best_val_loss     = None\n",
    "epochs_no_improve = 0\n",
    "lr                = 5e-4\n",
    "l1_factor         = 1e-5\n",
    "itr               =  tqdm(range(epochs))\n",
    "\n",
    "model_choice = \"idw\"\n",
    "\n",
    "if model_choice != \"idw\":\n",
    "    if model_choice == \"ffn\":\n",
    "        n_neurons = 64\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(2, n_neurons),\n",
    "            nn.BatchNorm1d(n_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons, n_neurons),\n",
    "            nn.BatchNorm1d(n_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons, n_neurons),\n",
    "            nn.BatchNorm1d(n_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons, 1)\n",
    "        ).to(device)\n",
    "\n",
    "    elif model_choice == \"polyfit\":\n",
    "        model = PolynomialRegressionModel(order=3, dim=2).to(device)\n",
    "        \n",
    "                                                             \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in itr:\n",
    "\n",
    "        if (epoch + 1) % int(epochs/10) == 0 or (epoch == 0):\n",
    "            \n",
    "            l1_regularization = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l1_regularization += torch.norm(param, 1)\n",
    "\n",
    "            train_loss  = criterion(model(train_X), train_Y) + l1_factor*l1_regularization\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss = criterion(model(val_X), val_Y) + l1_factor*l1_regularization\n",
    "\n",
    "            tqdm.write(f'Epoch {epoch+1}: Loss/Train: {train_loss.item():.4f}')\n",
    "            tqdm.write(f'Epoch {epoch+1}: Loss/Val: {val_loss.item():.4f}\\n')\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            l1_regularization = torch.tensor(0., requires_grad=True).to(device).float()\n",
    "            for param in model.parameters():\n",
    "                l1_regularization += torch.norm(param, 1).float()\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss    = criterion(outputs, y) + l1_factor*l1_regularization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[count] = loss.item()\n",
    "            count += 1\n",
    "\n",
    "        prev_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if current_lr != prev_lr:\n",
    "            tqdm.write(f\"Epoch {epoch+1}: Learning rate reduced from {prev_lr} to {current_lr}\")\n",
    "            prev_lr = current_lr\n",
    "\n",
    "        # Early stopping\n",
    "        if best_val_loss is None:\n",
    "            best_val_loss = val_loss.item()\n",
    "        elif val_loss.item() < best_val_loss:\n",
    "            epochs_no_improve = 0\n",
    "            best_val_loss = val_loss.item()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            tqdm.write(f\"Epoch {epoch+1}: Stopping training early\")\n",
    "            break\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    make_grid(axis=ax)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ax.plot(losses, alpha=0.5)\n",
    "else:\n",
    "    tmu = train_mu.squeeze()\n",
    "    tthet = train_theta.squeeze()\n",
    "    tlab = train_labels.squeeze()\n",
    "    model = lambda vec, x=tmu, y=tthet, z=tlab: inverse_distance_weighting(x, y, z, vec[0], vec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    if model_choice != \"idw\":\n",
    "        predictions = model(test_X).cpu().numpy()\n",
    "    else:\n",
    "        test_X_ = torch.hstack([test_mu, test_theta])\n",
    "        predictions = np.array([model(val) for val in test_X_])\n",
    "\n",
    "# Prepare the true values and predictions for plotting\n",
    "true_values  = test_Y.cpu().numpy()\n",
    "mu_values    = test_X[:, 0].cpu().numpy()\n",
    "theta_values = test_X[:, 1].cpu().numpy()\n",
    "\n",
    "# Create traces for true values and predictions\n",
    "trace_true = go.Scatter3d(\n",
    "    x=mu_values,\n",
    "    y=theta_values,\n",
    "    z=true_values.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color='blue',  # color for true values\n",
    "        opacity=0.25\n",
    "    ),\n",
    "    name='True Values'\n",
    ")\n",
    "\n",
    "trace_pred = go.Scatter3d(\n",
    "    x=mu_values,\n",
    "    y=theta_values,\n",
    "    z=predictions.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color='red',  # color for predictions\n",
    "        opacity=0.25\n",
    "    ),\n",
    "    name='Predictions'\n",
    ")\n",
    "\n",
    "data = [trace_true, trace_pred]\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='mu',\n",
    "        yaxis_title='theta',\n",
    "        zaxis_title='Quantile Value'\n",
    "    ),\n",
    "    margin=dict(r=0, b=0, l=0, t=0)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931f476",
   "metadata": {},
   "source": [
    "### 2. Training to learn the optimal scaling\n",
    "\n",
    "Steps are as follows, for each epoch:\n",
    "\n",
    "1. Sample N sets of $2M$ paths from $H_0$ and $M$ paths from $H_1$.\n",
    "2. Calculate a bootstrapped approximation to the MMD under $H_0$ and $H_1$ (biased)\n",
    "3. For the null distribution, approximate the critical value using statistics from the null distribution, use (learnt) Gamma approximation (must be FFN so you can backprop)\n",
    "4. Calculate smoothed Type II error expectation\n",
    "5. Backprop, repeat.\n",
    "\n",
    "We don't necessarily need to use the Gamma approximation - since the sort function in torch can be backpropagated through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25024bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0, sig = 0., 0.2\n",
    "mu1, beta = 0., 0.3\n",
    "noise_type = \"diagonal\"\n",
    "sde_type   = \"ito\"\n",
    "\n",
    "# Grid params\n",
    "T           = 1\n",
    "grid_points = 64\n",
    "dt_scale    = 1e-1  # Finer refinements give better solutions (but slower)\n",
    "ts          = torch.linspace(0, T, grid_points).to(device)\n",
    "\n",
    "# Path bank params\n",
    "path_bank_size = 32768\n",
    "state_size     = 1\n",
    "\n",
    "h0_model = ScaledBrownianMotion(mu0, sig, noise_type, sde_type).to(device)\n",
    "h1_model = ScaledBrownianMotion(mu1, beta, noise_type, sde_type).to(device)\n",
    "\n",
    "y0 = torch.full(size=(path_bank_size, state_size), fill_value=0.).to(device)\n",
    "\n",
    "_dt = dt_scale*torch.diff(ts)[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    h0_paths = torchsde.sdeint(h0_model, y0, ts, method='euler', dt = _dt)\n",
    "    h1_paths = torchsde.sdeint(h1_model, y0, ts, method='euler', dt = _dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_paths = torch.cat([ts.unsqueeze(-1).expand(path_bank_size, ts.size(0), 1), torch.transpose(h0_paths, 1, 0)], dim=2)\n",
    "h1_paths = torch.cat([ts.unsqueeze(-1).expand(path_bank_size, ts.size(0), 1), torch.transpose(h1_paths, 1, 0)], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyadic_order  = 1  # At least 1 for accurate calculation of the signature kernel\n",
    "static_kernel = sigkernel.LinearKernel()\n",
    "\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=dyadic_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826594a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiKernel(torch.nn.Module):\n",
    "    def __init__(self, initial_: torch.tensor):\n",
    "        super().__init__()\n",
    "        \n",
    "        device        = initial_.device\n",
    "        \n",
    "        n_scalings    = initial_.shape[0]\n",
    "        self.lambda_  = torch.nn.Parameter(initial_, requires_grad=True).to(device)\n",
    "        self.weights_ = (torch.ones(n_scalings)/n_scalings).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_step(x, scale_factor):\n",
    "    \"\"\"\n",
    "    Smooth approximation to step function.\n",
    "    \n",
    "    Parameters:\n",
    "        x (Tensor): Input tensor\n",
    "        threshold (float): The point around which to transition from 0 to 1\n",
    "        scale_factor (float): Controls the sharpness of the transition. Larger values make the transition sharper.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Smoothed step function applied to x\n",
    "    \"\"\"\n",
    "    return torch.sigmoid(scale_factor * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_biased_mmd(kernel, mu, nu, max_batch=128):\n",
    "    K_XX = kernel.compute_Gram(mu, mu, sym=True, max_batch=max_batch)\n",
    "    K_YY = kernel.compute_Gram(nu, nu, sym=True, max_batch=max_batch)\n",
    "    K_XY = kernel.compute_Gram(mu, nu, sym=False, max_batch=max_batch)\n",
    "    \n",
    "    return K_XX.mean() + K_YY.mean() - 2. * torch.mean(K_XY)\n",
    "\n",
    "def expected_type2_error(dist, crit_value):\n",
    "    n_atoms = dist.shape[0]\n",
    "    num_fail = dist <= crit_value\n",
    "    return sum(num_fail)/n_atoms\n",
    "\n",
    "def soft_quantile(v, tau, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Approximate a quantile of a vector `v` in a differentiable way.\n",
    "    - v: Input tensor (vector).\n",
    "    - tau: Desired quantile (0 to 1).\n",
    "    - temperature: Temperature for the softmax, controls smoothness.\n",
    "    \"\"\"\n",
    "    n = v.size(0)\n",
    "    ranks = torch.arange(1, n+1, device=v.device).float()\n",
    "    target_rank = tau * n\n",
    "    # Compute distances to the desired rank, negative distances for differentiation\n",
    "    distances = -(ranks - target_rank).abs()\n",
    "    # Compute soft weights\n",
    "    weights = F.softmax(distances / temperature, dim=0)\n",
    "    # Compute weighted sum as the quantile approximation\n",
    "    quantile_approx = torch.sum(weights * v)\n",
    "    return quantile_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin optimisation test\n",
    "# model.eval()\n",
    "n_steps    = 256\n",
    "tr_loss    = torch.zeros(n_steps).to(device)\n",
    "\n",
    "n_paths = 16\n",
    "n_atoms = 64\n",
    "alpha   = 0.05\n",
    "n_scalings = 2\n",
    "\n",
    "initial_   = torch.tensor([1. for _ in range(n_scalings)]).to(device)\n",
    "lr         = 5e-2\n",
    "redr_flag  = True\n",
    "optimizer_ = \"Adam\"\n",
    "lambdas_   = torch.zeros((n_steps, n_scalings)).to(device)\n",
    "\n",
    "scale_factor = 1e4\n",
    "method_      = \"quantile\"\n",
    "tau          = 1-alpha\n",
    "\n",
    "dyadic_order     = 0  # At least 1 for accurate calculation of the signature kernel\n",
    "static_kernel    = sigkernel.LinearKernel()\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=dyadic_order)\n",
    "\n",
    "phi                = PhiKernel(initial_).to(device)\n",
    "time_normalisation = True\n",
    "optimizer          = getattr(torch.optim, optimizer_)(phi.parameters(), lr=lr)\n",
    "\n",
    "scale = lambda mean, var, N: mean**2/var\n",
    "rate  = lambda mean, var, N: (N*var)/mean\n",
    "\n",
    "scaler_means = torch.tensor(scaler.mean_, dtype=torch.float).to(device)\n",
    "scaler_stds = torch.tensor(scaler.scale_, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e1075",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "trange_steps = tqdm(range(n_steps), position=0)\n",
    "\n",
    "for step in trange_steps:\n",
    "    \n",
    "    # Init mmds\n",
    "    mmd_h0 = torch.zeros((n_atoms, n_scalings)).to(device)\n",
    "    mmd_h1 = torch.zeros((n_atoms, n_scalings)).to(device)\n",
    "    \n",
    "    # Init loss\n",
    "    loss = 0\n",
    "    \n",
    "    # Calculate distributions for each scaling\n",
    "    for k, lambd_ in enumerate(phi.lambda_):\n",
    "        \n",
    "        t_h0_paths = lambd_*h0_paths.clone().to(device)\n",
    "        t_h1_paths = lambd_*h1_paths.clone().to(device)\n",
    "        \n",
    "        if time_normalisation:\n",
    "            t_h0_paths[..., 0] /= lambd_*T\n",
    "            t_h1_paths[..., 0] /= lambd_*T\n",
    "\n",
    "        for j in range(n_atoms):\n",
    "            # Generate random noise\n",
    "            h0_rands = torch.randperm(path_bank_size)[:int(2*n_paths)]\n",
    "\n",
    "            ix, jx = h0_rands[:n_paths], h0_rands[n_paths:]\n",
    "            iy     = torch.randperm(path_bank_size)[:n_paths]\n",
    "            \n",
    "            # mmd_h0[j, k] = compute_biased_mmd(t_h0_paths[ix], t_h0_paths[jx])\n",
    "            # mmd_h1[j, k] = compute_biased_mmd(t_h0_paths[ix], t_h1_paths[iy])\n",
    "            \n",
    "            mmd_h0[j, k] = compute_biased_mmd(signature_kernel, t_h0_paths[ix], t_h0_paths[jx])\n",
    "            mmd_h1[j, k] = compute_biased_mmd(signature_kernel, t_h0_paths[ix], t_h1_paths[iy])\n",
    "            \n",
    "    # Flatten the MMD values \n",
    "    mmd_h0_phi = torch.matmul(mmd_h0, phi.weights_)\n",
    "    mmd_h1_phi = torch.matmul(mmd_h1, phi.weights_)\n",
    "    \n",
    "    ### This is where it gets tricky\n",
    "    if method_ == \"gamma\":\n",
    "        # Use Gamma approximation as differentiable surrogate\n",
    "        mean = mmd_h0_phi.mean()\n",
    "        var  = mmd_h0_phi.var()\n",
    "\n",
    "        mu    = scale(mean, var, n_paths).unsqueeze(-1)\n",
    "        theta = rate(mean, var, n_paths).unsqueeze(-1)\n",
    "\n",
    "        inputs = torch.hstack([mu, theta])\n",
    "\n",
    "        # Transform the inputs using the scaler\n",
    "        inputs_scaled = (inputs - scaler_means)/scaler_stds\n",
    "\n",
    "        # Get critical threshold\n",
    "        crit_thresh = model(inputs_scaled.unsqueeze(0))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            is_cpu = inputs_scaled.cpu()\n",
    "            trange_steps.write(f\"Scaled input values: mu: {is_cpu[0]:.4f}, theta: {is_cpu[1]:.4f}\")\n",
    "            trange_steps.write(f\"Estimated critical threshold: {crit_thresh.cpu()[0][0]: .4f}\")\n",
    "            trange_steps.write(f\"True critical threshold: {gamma_ppf(1-alpha, mu.cpu(), theta.cpu())[0]: .4f}\")\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = 1 - smooth_step(n_paths*mmd_h1_phi - crit_thresh, scale_factor).mean()\n",
    "        \n",
    "    elif method_ == \"comparison\":\n",
    "        loss = torch.mean(smooth_step(mmd_h0_phi - mmd_h1_phi, scale_factor))\n",
    "    elif method_ == \"soft_quantile\":\n",
    "        crit_val = soft_quantile(mmd_h0_phi, tau, temperature=1.0)\n",
    "        loss = smooth_step(crit_val - mmd_h1_phi, scale_factor).mean()\n",
    "    elif method_ == \"quantile\":\n",
    "        crit_val = torch.quantile(mmd_h0_phi, 0.95)\n",
    "        loss = smooth_step(crit_val - mmd_h1_phi, scale_factor).mean()\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        trange_steps.write(\"NaN values found in output\")\n",
    "        break\n",
    "        \n",
    "    if loss == 0:\n",
    "        trange_steps.write(\"Loss reached minimum. Exiting\")\n",
    "        break\n",
    "    \n",
    "    if (loss < 0.3) and redr_flag:\n",
    "        redr_flag = False\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "    \n",
    "    if (step + 1) % 16 == 0:\n",
    "        trange_steps.write(f\"Epoch {step+1}: Current loss: {loss: .4f}\")\n",
    "        with torch.no_grad():\n",
    "            fprint_lambdas_ = phi.lambda_.data\n",
    "            trange_steps.write(f\"Current scalings: {', '.join([str(l.item())[:5] for l in fprint_lambdas_])}\")\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        phi.lambda_.data = torch.clamp(phi.lambda_.data, min=1e-2, max=1e4)\n",
    "    \n",
    "    # Update \n",
    "    tr_loss[step]  = loss.item()\n",
    "    lambdas_[step] = phi.lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "with torch.no_grad():\n",
    "    ax1.plot(tr_loss.cpu().numpy(), color=\"dodgerblue\", alpha=0.75, label=\"loss\")\n",
    "    ax2.plot(lambdas_.squeeze(-1).cpu(), color=\"tomato\", alpha=0.75, label=\"$\\lambda_1$\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    make_grid(axis=ax1)\n",
    "    make_grid(axis=ax2)\n",
    "\n",
    "ax1.set_title(f\"Loss, batch size ${n_paths}$, ${n_atoms}$ atoms\", fontsize=\"medium\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Type II Error\")\n",
    "ax2.set_title(f\"Scalings, number = ${n_scalings}$\", fontsize=\"medium\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Scaling\")\n",
    "fig.suptitle(\"Learning optimal scaling, BM$(0.2)$ vs BM$(0.3)$\");\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"optimal_scaling_loss.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463406b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse and find a good scaling, brute force for now, to guide the optimization\n",
    "n_scalings = 32\n",
    "scalings = torch.linspace(1e-2, 3e1, n_scalings)\n",
    "n_atoms = 128\n",
    "n_paths = 32\n",
    "\n",
    "kernels = [sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=i) for i in range(3)]\n",
    "\n",
    "res = torch.zeros((3, n_scalings))\n",
    "time_normalisation = True\n",
    "\n",
    "for k, kernel in enumerate(kernels):\n",
    "    for j, lambd_ in enumerate(tqdm(scalings)):\n",
    "        t_h0_paths = lambd_*h0_paths.clone().to(device)\n",
    "        t_h1_paths = lambd_*h1_paths.clone().to(device)\n",
    "\n",
    "        if time_normalisation:\n",
    "            t_h0_paths[..., 0] /= lambd_*T\n",
    "            t_h1_paths[..., 0] /= lambd_*T\n",
    "\n",
    "        mmd_h0 = torch.zeros(n_atoms)\n",
    "        mmd_h1 = torch.zeros(n_atoms)\n",
    "        for i in range(n_atoms):\n",
    "            h0_rands = torch.randperm(path_bank_size)[:int(2*n_paths)]\n",
    "\n",
    "            ix, jx = h0_rands[:n_paths], h0_rands[n_paths:]\n",
    "            iy     = torch.randperm(path_bank_size)[:n_paths]\n",
    "\n",
    "            mmd_h0[i] = compute_biased_mmd(kernel, t_h0_paths[ix], t_h0_paths[jx])\n",
    "            mmd_h1[i] = compute_biased_mmd(kernel, t_h0_paths[ix], t_h1_paths[iy])\n",
    "\n",
    "        crit_val  = mmd_h0.sort()[0][int(n_atoms*(0.95))]\n",
    "        res[k, j] = expected_type2_error(mmd_h1, crit_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "colors = sns.diverging_palette(250, 20, n=3, center=\"dark\")\n",
    "for i, r in enumerate(res):\n",
    "    ax.plot(scalings, r, alpha=0.5, label=f\"dyadic_order_{i}\", color=colors[i])\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
