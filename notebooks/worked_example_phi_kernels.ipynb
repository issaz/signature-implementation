{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Type II error for the generalized signature kernel MMD with examples (Gaussian processes)\n",
    "\n",
    "## Z Issa May 2023\n",
    "\n",
    "In this notebook, we provide code and worked examples for studying the Type II error present when calculating unbiased estimates for the signature kernel MMD (for fixed sample size $N \\in \\mathbb{N}$) between two sets of paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torchsde\n",
    "import sigkernel\n",
    "import iisignature\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.utils.helper_functions.plot_helper_functions import make_grid, golden_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if is_cuda else 'cpu'\n",
    "\n",
    "if not is_cuda:\n",
    "    print(\"Warning: CUDA not available; falling back to CPU but this is likely to be very slow.\")\n",
    "    \n",
    "# You realistically need GPU access (either natively or via cloud computing) to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some gbm paths\n",
    "class GeometricBrownianMotion(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, mu, sigma, noise_type: str, sde_type: str):\n",
    "        super().__init__()\n",
    "        self.mu      = torch.nn.Parameter(torch.tensor(mu, dtype=torch.float64), requires_grad=True) \n",
    "        self.sigma   = torch.nn.Parameter(torch.tensor(sigma, dtype=torch.float64), requires_grad=True)\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type   = sde_type\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        return self.mu*y\n",
    "    \n",
    "    def g(self, t, y):\n",
    "        return self.sigma*y\n",
    "\n",
    "class BrownianMotionDrift(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, mu, sigma, noise_type: str, sde_type: str):\n",
    "        super().__init__()\n",
    "        self.mu         = torch.nn.Parameter(torch.tensor(mu, dtype=torch.float64), requires_grad=True) \n",
    "        self.sigma      = torch.nn.Parameter(torch.tensor(sigma, dtype=torch.float64), requires_grad=True)\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type   = sde_type\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        return torch.zeros(size=y.size()).to(y.device) + self.mu\n",
    "    \n",
    "    def g(self, t, y):\n",
    "        return torch.zeros(size=y.size()).to(y.device) + self.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sde params\n",
    "mu0, sig = 0., 0.2\n",
    "mu1, beta = 0., 0.3\n",
    "noise_type = \"diagonal\"\n",
    "sde_type   = \"ito\"\n",
    "\n",
    "# Grid params\n",
    "T           = 1\n",
    "grid_points = 32\n",
    "dt_scale    = 1e-1  # Finer refinements give better solutions (but slower)\n",
    "ts          = torch.linspace(0, T, grid_points).to(device)\n",
    "\n",
    "# Path bank params\n",
    "\n",
    "batch_size = 32768\n",
    "state_size = 1\n",
    "\n",
    "h0_model = BrownianMotionDrift(mu0, sig, noise_type, sde_type).to(device)\n",
    "h1_model = BrownianMotionDrift(mu1, beta, noise_type, sde_type).to(device)\n",
    "\n",
    "y0 = torch.full(size=(batch_size, state_size), fill_value=0.).to(device)\n",
    "\n",
    "_dt = dt_scale*torch.diff(ts)[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    h0_paths = torchsde.sdeint(h0_model, y0, ts, method='euler', dt = _dt)\n",
    "    h1_paths = torchsde.sdeint(h1_model, y0, ts, method='euler', dt = _dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_paths = torch.cat([ts.unsqueeze(-1).expand(batch_size, ts.size(0), 1), torch.transpose(h0_paths, 1, 0)], dim=2)\n",
    "h1_paths = torch.cat([ts.unsqueeze(-1).expand(batch_size, ts.size(0), 1), torch.transpose(h1_paths, 1, 0)], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to verify \n",
    "n_plot_paths = 32 \n",
    "label_first = True\n",
    "\n",
    "for p0, p1 in zip(h0_paths[:n_plot_paths].cpu(), h1_paths[:n_plot_paths].cpu()):\n",
    "    plt.plot(ts.cpu(), p0[:, 1], color=\"dodgerblue\", alpha=0.5, label=\"$H_0$\" if label_first else \"\")\n",
    "    plt.plot(ts.cpu(), p1[:, 1], color=\"tomato\", alpha=0.5, label=\"$H_1$\" if label_first else \"\")\n",
    "    label_first = False\n",
    "plt.legend()\n",
    "make_grid()\n",
    "plt.title(fr\"$H_0$ vs $H_1$, scaled BM, $\\sigma = {sig:.2f}$, $\\beta = {beta:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample $H_0$ and $H_1$ MMD distributions, define Type II error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mmd_distributions(h0_paths, h1_paths, estimator, n_atoms=128, n_paths=32):\n",
    "\n",
    "    h0_dists = torch.zeros(n_atoms)\n",
    "    h1_dists = torch.zeros(n_atoms)\n",
    "\n",
    "    rand_ints = torch.randint(0, batch_size, size=(n_atoms, 3, n_paths))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, ii in enumerate(tqdm(rand_ints)):\n",
    "            x1, x2 = h0_paths[ii[0]], h0_paths[ii[1]]\n",
    "            y = h1_paths[ii[-1]]\n",
    "\n",
    "            h0_dists[i] = estimator(x1, x2)\n",
    "            h1_dists[i] = estimator(x1, y)\n",
    "            \n",
    "    return h0_dists, h1_dists\n",
    "\n",
    "def expected_type2_error(dist, crit_value):\n",
    "    n_atoms = dist.shape[0]\n",
    "    num_fail = dist <= crit_value\n",
    "    return sum(num_fail)/n_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_transform(path: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Returns the path of absolute increments Y associated to a path X. This is given by Y_0 = X_0 and\n",
    "    Y_i = Y_{i-1} + |X_i - X_{i-1}|.\n",
    "\n",
    "    :param path:    Path to calculate increment transform of.\n",
    "    :return:        Transformed path.\n",
    "    \"\"\"\n",
    "    device = path.device\n",
    "    N, l, d = path.shape\n",
    "    \n",
    "    res = torch.zeros(path.shape).to(device)\n",
    "    res[..., 0] = path[..., 0]\n",
    "    res[..., 1:, 1:] = torch.abs(path[..., 1:].diff(axis=1)).cumsum(dim=1)\n",
    "    \n",
    "    res[..., 1:] += torch.tile(path[:, 0, 1:].unsqueeze(-1), (1, l, 1))\n",
    "\n",
    "    return res\n",
    "\n",
    "def scale_transform(path: torch.tensor, scaler: torch.float32) -> torch.tensor:\n",
    "    device = path.device\n",
    "    res    = torch.zeros(path.shape).to(device)\n",
    "    \n",
    "    scaler_ = torch.tensor(scaler).to(device)\n",
    "    \n",
    "    res[..., 1:] = path[..., 1:]*scaler_\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize signature kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We work with the linear kernel (for now), a la the paper. \n",
    "\n",
    "dyadic_order  = 1  # At least 1 for accurate calculation of the signature kernel\n",
    "static_kernel = sigkernel.LinearKernel()\n",
    "\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=dyadic_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atoms   = 1024\n",
    "n_paths   = 64\n",
    "max_batch = 64\n",
    "_scale_f  = torch.tensor(n_paths).sqrt()\n",
    "alpha     = 0.05\n",
    "time_normalisation = True\n",
    "_scaler = 1 \n",
    "\n",
    "t_h0_paths = scale_transform(h0_paths.clone(), _scaler)\n",
    "t_h1_paths = scale_transform(h1_paths.clone(), _scaler)\n",
    "estimator = lambda x, y: signature_kernel.compute_mmd(x, y, max_batch=max_batch)\n",
    "\n",
    "if time_normalisation:\n",
    "    t_h0_paths[..., 0] /= _scaler*T\n",
    "    t_h1_paths[..., 0] /= _scaler*T\n",
    "\n",
    "h0_dists, h1_dists = return_mmd_distributions(\n",
    "    t_h0_paths, \n",
    "    t_h1_paths, \n",
    "    estimator,\n",
    "    n_atoms=n_atoms, \n",
    "    n_paths=n_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_val = h0_dists.sort()[0][int(n_atoms*(1-alpha))]\n",
    "\n",
    "plt.figure(figsize=golden_dimensions(4))\n",
    "n_bins = int(n_atoms/8)\n",
    "plt.title(f\"{n_paths} paths, {n_atoms} atoms. Expected Type II error: {100*expected_type2_error(h1_dists, crit_val):.2f}%\", fontsize=\"small\")\n",
    "plt.hist(sorted(h0_dists)[16:-16], bins=n_bins, color=\"dodgerblue\", alpha=0.5, label=\"$H_0$\", density=True)\n",
    "plt.hist(sorted(h1_dists)[16:-16], bins=n_bins, color=\"tomato\"    , alpha=0.5, label=\"$H_1$\", density=True)\n",
    "plt.legend()\n",
    "make_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get level-$k$ signature term contributions\n",
    "\n",
    "Suppose $\\mathcal{X}$ is a compact subset of $\\mathcal{C}_p([0, T]; V)$, the set of unparametrized, tree-reduced paths with finite $p$-variation for $1 \\le p < 2$ (i.e., the set of equivalence classes of paths of finite $p$-variation where the equivalence relation $\\sim_\\tau$ denotes tree-like equivalence). Let $S: \\mathcal{X} \\to T((V))$ denote the signature mapping and $S(X)^k$ the terms corresponding to the $k^{\\text{th}}$ level signature.\n",
    "\n",
    "Let $\\mathcal{D}_{k_\\phi}: \\mathcal{P}(\\mathcal{X}) \\times \\mathcal{P}(\\mathcal{X}) \\to \\mathbb{R}$ denote the maximum mean discrepancy with the $\\phi$-siganture kernel. Because we know that\n",
    "\n",
    "$$ k_\\phi(X, Y) = \\sum_{k\\ge 0}\\phi(k) \\langle S(X)^k, S(Y)^k \\rangle_k, $$\n",
    "\n",
    "where $\\langle \\cdot, \\cdot \\rangle_k = \\langle \\cdot, \\cdot \\rangle_{V^{\\otimes k}}$ and thus $\\langle A, B \\rangle_{T((V))} = \\sum_{k\\ge 0} \\langle A^k, B^k \\rangle_k$, the Hilbert-Schmidt norm. Then we know that \n",
    "\n",
    "$$\\mathcal{D}_{k_\\phi}(\\mathbb{P}, \\mathbb{Q})^2 = \\mathbb{E}_{\\mathbb{P}}[k_\\phi(X, X')] - 2\\mathbb{E}_{\\mathbb{P}, \\mathbb{Q}}[k_\\phi(X, Y)] + \\mathbb{E}_{\\mathbb{Q}}[k_\\phi(Y, Y')]$$\n",
    "\n",
    "and thus \n",
    "\n",
    "$$\\mathcal{D}_{k_\\phi}(\\mathbb{P}, \\mathbb{Q})^2 = \\phi(0) + \\sum_{k\\ge 1}\\phi(k) \\left[\\mathbb{E}\\Lambda_k(\\mathbb{P}, \\mathbb{P}) - 2\\mathbb{E}\\Lambda_k(\\mathbb{P}, \\mathbb{Q}) + \\mathbb{E}\\Lambda_k(\\mathbb{Q}, \\mathbb{Q}) \\right]$$\n",
    "\n",
    "where $\\mathbb{E}\\Lambda_k: \\mathcal{P}(\\mathcal{X}) \\times \\mathcal{P}(\\mathcal{X}) \\to \\mathbb{R}$ is defined by $$\\mathbb{E}\\Lambda_k(\\mathbb{P}, \\mathbb{Q}) = \\sum_{i_1,\\dots,i_k \\in \\mathcal{W}(V^{\\otimes k})} \\mathbb{E}_\\mathbb{P}[S(X)^{i_1,\\dots,i_k}]\\mathbb{E}_\\mathbb{Q}[S(Y)^{i_1,\\dots,i_k}].$$\n",
    "\n",
    "This means we can view the (squared) MMD as the sum of (weighted) level contributions. Let's reconstruct these now for this example.\n",
    "\n",
    "<b>Remark</b>: Here we can explicitly evaluate the $\\phi$-function $\\phi: \\mathbb{N} \\cup \\{0\\} \\to \\mathbb{R}$. In practice, you cannot, as one does not calculate terms directly but rather solves a PDE to calculate the signature kernel. However, if a desired $\\phi$ solves the Hamburger moment problem, $$\\phi(k) = \\int_{\\Omega} \\pi^k \\,d\\mu(\\pi)$$ for a probability measure $\\mu \\in \\mathcal{P}(\\mathbb{R})$, then we can calculate the $\\phi$-kernel as a weighted sum of $k_{\\lambda}$-kernels (where $\\lambda \\sim \\mu$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signatures(paths, k):\n",
    "    # Calculate the signature up to the required order\n",
    "    \n",
    "    n, l, d = paths.shape\n",
    "        \n",
    "    if k == 0:\n",
    "        return torch.zeros(n) + 1.\n",
    "    \n",
    "    if type(paths) == torch.Tensor:\n",
    "        paths = np.array(paths.cpu().detach())\n",
    "        \n",
    "    sigs  = iisignature.sig(paths, k)\n",
    "    return torch.tensor(sigs)\n",
    "\n",
    "def get_level_k_signatures(paths, k):\n",
    "    # Calculate the signature up to the required order\n",
    "    _, _, d= paths.shape\n",
    "    \n",
    "    sigs  = get_signatures(paths, k)\n",
    "    stloc = sum((d**ki for ki in range(k))) - 1\n",
    "    \n",
    "    return sigs[:, stloc:]\n",
    "\n",
    "def get_level_k_signatures_from_signatures(signatures, k, d):\n",
    "    \"\"\"\n",
    "    Extracts the level k terms from the bank of signatures directly. Requires more data in order to get the correct\n",
    "    indexing\n",
    "\n",
    "    :param signatures:      Bank of signatures\n",
    "    :param k:               Order of level terms to extract\n",
    "    :param d:               Dimension of path\n",
    "    :return:                Level k signature terms\n",
    "    \"\"\"\n",
    "\n",
    "    stloc = sum((d ** ki for ki in range(k))) - 1\n",
    "    endloc = sum((d ** ki for ki in range(k+1))) - 1\n",
    "\n",
    "    return signatures[:, stloc:endloc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batched samples $X=\\{X_i\\}_{i=1}^N, Y=\\{Y_i\\}_{i=1}^N$, the (unbiased) MMD estimator can be re-written as \n",
    "\n",
    "$$ D_{k_\\phi}^u(X, Y)^2 = \\sum_{k\\ge 0} \\left(\\frac{1}{N(N-1)}\\sum_{i\\ne j}\\phi(k)\\left[\\Lambda_k(X_i, X_j) + \\Lambda_k(Y_i, Y_j)\\right] - \\frac{2}{N^2}\\sum_{i,j=1}^N \\phi(k)\\Lambda_k(X_i, Y_j)\\right),$$\n",
    "\n",
    "where $\\Lambda_k(X, Y) = \\mathbb{E}\\Lambda_k(\\delta_X, \\delta_Y)$. Thus the level-$k$ contribution is given by \n",
    "\n",
    "$$M_k(X, Y) = \\frac{1}{N(N-1)}\\sum_{i\\ne j}\\phi(k)\\left[\\Lambda_k(X_i, X_j) + \\Lambda_k(Y_i, Y_j)\\right] - \\frac{2}{N^2}\\sum_{i,j=1}^N \\phi(k)\\Lambda_k(X_i, Y_j).$$\n",
    "\n",
    "Written in matrix form, \n",
    "\n",
    "$$M_k(X, Y) = \\phi(k) \\left(\\frac{1}{N(N-1)}(\\mathbb{E}[G^u_{XX}] + \\mathbb{E}[G^u_{YY}]) - \\frac{2}{N^2}\\mathbb{E}[G_{XY}]\\right), $$\n",
    "\n",
    "where $G_{XY}$ is the Gram matrix between $X, Y$, superscript $u$ denotes unbiased (diagonals removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lambda_k(X, Y, k):\n",
    "    \"\"\"Calculates lambda_k given two sets of paths, shape N x l x d\"\"\"\n",
    "    \n",
    "    if k == 0:\n",
    "        return 1.\n",
    "    \n",
    "    sigX = torch.mean(get_level_k_signatures(X, k), dim=0)\n",
    "    sigY = torch.mean(get_level_k_signatures(Y, k), dim=0)\n",
    "    \n",
    "    return torch.dot(sigX, sigY)\n",
    "\n",
    "def Gramda_k(X, Y, k):\n",
    "    N = X.size(0)\n",
    "    \n",
    "    sigsX = get_level_k_signatures(X, k)\n",
    "    sigsY = get_level_k_signatures(Y, k)\n",
    "    \n",
    "    return torch.einsum(\"ik,jk->ij\", sigsX, sigsY)\n",
    "\n",
    "def M_k(X, Y, k, phik=lambda x: 1):\n",
    "    if k == 0:\n",
    "        return 0.\n",
    "    \n",
    "    N = X.size(0)\n",
    "    \n",
    "    gXX = Gramda_k(X, X, k)\n",
    "    gYY = Gramda_k(Y, Y, k)\n",
    "    gXY = Gramda_k(X, Y, k)\n",
    "\n",
    "    gXX -= torch.diag(torch.diag(gXX))\n",
    "    gYY -= torch.diag(torch.diag(gYY))\n",
    "\n",
    "    \n",
    "    res1 = (torch.sum(gXX) + torch.sum(gYY))/(N*(N-1))\n",
    "    res2 = 2*torch.sum(gXY)/(N**2)\n",
    "    \n",
    "    return phik(k)*(res1 - res2)\n",
    "\n",
    "\n",
    "def kernel_est_k(X, Y, k, phik=lambda x: 1):\n",
    "    return torch.sum(torch.tensor([phik(_ki)*Lambda_k(_x, _x, _ki) for _ki in range(k)]))\n",
    "\n",
    "def mmd_est_k(X, Y, k, phik=lambda x: 1):\n",
    "    return torch.sum(torch.tensor([M_k(X, Y, ki, phik) for ki in range(k)]), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions \n",
    "_x = h0_paths[0, :, :].unsqueeze(0)\n",
    "max_k = 10\n",
    "\n",
    "true_kernel = signature_kernel.compute_kernel(_x, _x)\n",
    "lambdak_est = kernel_est_k(_x, _x, max_k)\n",
    "\n",
    "print(f\"True kernel value: {true_kernel.item():.4f}. Lambda_k estimate (lvl {max_k}): {lambdak_est.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MMD estimator\n",
    "torch.cuda.empty_cache()\n",
    "n_paths = 32\n",
    "\n",
    "testX = h0_paths[:n_paths]\n",
    "testY = h1_paths[:n_paths]\n",
    "\n",
    "true_mmd = signature_kernel.compute_mmd(testX, testY)\n",
    "\n",
    "print(f\"True MMD: {true_mmd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_mmd = mmd_est_k(testX, testY, max_k)\n",
    "\n",
    "print(f\"Estimate MMD: {estimate_mmd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plot \n",
    "_conts = np.array([M_k(testX, testY, _k) for _k in range(max_k)]).cumsum()\n",
    "\n",
    "plt.figure(figsize=golden_dimensions(4))\n",
    "\n",
    "#plt.title(f\"Unbiased sample MMD vs level-$k$ estimate MMD convergence ($N={n_paths}$)\", fontsize=\"small\")\n",
    "n_bins = int(n_atoms/8)\n",
    "plt.axhline(true_mmd.cpu(), linestyle=\"dashed\", color=\"tomato\", alpha=0.75, label=r\"$\\mathcal{D}_{k_{\\mathrm{Sig}}}(\\mathbf{X}, \\mathbf{Y})^2$\")\n",
    "plt.plot(np.arange(max_k), _conts, color=\"dodgerblue\", alpha=0.5, label=r\"$\\sum_{k> 0}^n M_k(\\mathbf{X}, \\mathbf{Y})$\")\n",
    "plt.legend()\n",
    "make_grid()\n",
    "# plt.savefig(\"convergence_to_true_phik_1.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Study the differences in signature terms \n",
    "\n",
    "The main idea presented here is that, in some cases, the signature terms that differ the most tend to come from the higher orders of the signature. Thus, to minimise the type II error, these terms will need to be more pronounced in the MMD calculation than those of lower order. We first study the different in distribution between these terms to gain a qualitative understanding of how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 5\n",
    "\n",
    "h0_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "h1_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "\n",
    "rand_ints = torch.randint(0, batch_size, size=(n_atoms, 3, n_paths))\n",
    "\n",
    "for i, ii in enumerate(tqdm(rand_ints)):\n",
    "    # Sample some paths\n",
    "    \n",
    "    x1, x2 = h0_paths[ii[0]], h0_paths[ii[1]]\n",
    "    y      = h1_paths[ii[-1]]\n",
    "    \n",
    "    for j, _k in enumerate(range(1, max_k + 1)):  # 0 order is not interesting\n",
    "        # Calculate level-k contribution\n",
    "        h0_cont_dists[j, i] = M_k(x1, x2, _k)\n",
    "        h1_cont_dists[j, i] = M_k(x1, y, _k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot side-by-side\n",
    "fig, ax = plt.subplots(1, max_k, figsize=(max_k*5, 5))\n",
    "\n",
    "n_bins = int(n_atoms/8)\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.hist(h0_cont_dists[i], bins = n_bins, color=\"dodgerblue\", alpha=0.5, label=\n",
    "             f\"h0\", density=True)\n",
    "    axi.hist(h1_cont_dists[i], bins = n_bins, color=\"tomato\"    , alpha=0.5, label=f\"h1\", density=True)\n",
    "    make_grid(axis=axi)\n",
    "    axi.legend(fontsize=\"small\")\n",
    "    axi.set_title(f\"Level {i+1}\")\n",
    "#fig.suptitle(\"Distribution of level contributions\");\n",
    "plt.savefig(\"levelcontributionsbase.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we don't start to get significant deviation in the distribution of the level contributions under $H_1$ until the level is quite high. However, by then, as from the previous graph, this is only adding a very marginal amount to the value of the MMD. \n",
    "\n",
    "We can illustrate this with a separate graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the positions of each histogram\n",
    "n_bins = int(n_atoms/8)\n",
    "\n",
    "x_positions = np.arange(max_k+1)[::-1]\n",
    "\n",
    "plot_dat        = np.zeros((2, max_k + 1, n_atoms))\n",
    "\n",
    "plot_dat[0, 0]  = h0_dists\n",
    "plot_dat[0, 1:] = h0_cont_dists\n",
    "\n",
    "plot_dat[1, 0]  = h1_dists\n",
    "plot_dat[1, 1:] = h1_cont_dists\n",
    "\n",
    "# Plot the histograms\n",
    "colors= [\"dodgerblue\", \"tomato\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for j, dat in enumerate(plot_dat):\n",
    "    for i, dat_ in enumerate(dat):\n",
    "        hist, bin_edges = np.histogram(dat_, bins=n_bins, density=False)\n",
    "        bin_centers     = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        width = np.diff(bin_centers)[0]\n",
    "        \n",
    "        axes[j].bar(bin_centers, hist, zs=x_positions[i], zdir='y', alpha=0.8, width=width, color=colors[j])\n",
    "        axes[j].set_xlabel(\"$M_k$\")\n",
    "        \n",
    "        yticks      = x_positions + 1\n",
    "        yticklabels = [\"$D(P,Q)^2$\"] + [f\"level-{k}\" for k in range(1, max_k+1)]\n",
    "        \n",
    "        axes[j].set_yticks(yticks)\n",
    "        axes[j].set_yticklabels(yticklabels)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Contribution of signature terms, null and alternate hypotheses\");\n",
    "# plt.savefig(\"levelcontributionhistsbase.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling to minimise the type II error\n",
    "\n",
    "We now want to see if we can find a scaling such that the contributions from the higher-order terms matter more.\n",
    "\n",
    "### 7.1 Removing factorial decay: $\\phi(k) = (k/2)!$ \n",
    "\n",
    "As suggested, we can try to remove the factorial decay by choosing $\\pi \\sim \\text{Exp}(1)$, which has moment function $$\\phi(k) = \\int \\pi^k\\, d\\mu(\\pi) = \\int x^k e^{-x}\\, dx = \\Gamma(k+1) = k!,$$\n",
    "\n",
    "so (in the PDE setting) in order to arrive at the $\\phi$-kernel, we need to do \n",
    "\n",
    "\\begin{equation*}\n",
    "    k_\\phi(X, Y) \\approxeq \\frac{1}{M}\\sum_{i=1}^M k(\\pi^{1/2}X, Y), \n",
    "\\end{equation*}\n",
    "where $\\pi \\sim \\text{Exp}(1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiKernel(torch.nn.Module):\n",
    "    def __init__(self, dyadic_order, n_scalings, max_batch=32):\n",
    "        super().__init__()\n",
    "        self.sigkernel  = sigkernel.SigKernel(static_kernel=sigkernel.LinearKernel(), dyadic_order=dyadic_order)\n",
    "        self.scalings   = torch.zeros(n_scalings).exponential_()\n",
    "        self.n_scalings = n_scalings\n",
    "        self.max_batch  = max_batch\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        res = 0\n",
    "        for pi in self.scalings:\n",
    "            res += self.sigkernel.compute_kernel(torch.sqrt(pi)*X, Y, max_batch=self.max_batch)\n",
    "            \n",
    "        return res/self.n_scalings\n",
    "    \n",
    "    def compute_mmd(self, X, Y):\n",
    "        res = 0\n",
    "        for pi in self.scalings:\n",
    "            K_XX = self.sigkernel.compute_Gram(torch.sqrt(pi)*X, X, sym=False, max_batch=self.max_batch)\n",
    "            K_XY = self.sigkernel.compute_Gram(torch.sqrt(pi)*X, Y, sym=False, max_batch=self.max_batch)\n",
    "            K_YY = self.sigkernel.compute_Gram(torch.sqrt(pi)*Y, Y, sym=False, max_batch=self.max_batch)\n",
    "            \n",
    "            K_XX_m = (torch.sum(K_XX) - torch.sum(torch.diag(K_XX))) / (K_XX.shape[0] * (K_XX.shape[0] - 1.))\n",
    "            K_YY_m = (torch.sum(K_YY) - torch.sum(torch.diag(K_YY))) / (K_YY.shape[0] * (K_YY.shape[0] - 1.))\n",
    "            \n",
    "            res += K_XX_m + K_YY_m - 2. * torch.mean(K_XY)\n",
    "        return res/self.n_scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_kernel = PhiKernel(dyadic_order=1, n_scalings=25, max_batch=32)\n",
    "phik       = lambda x: torch.exp(torch.lgamma(torch.tensor(x/2 + 1)))\n",
    "estimator  = lambda x, y, tk=10, pk=phik: mmd_est_k(x, y, k=tk, phik=pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fac_h0_dists, fac_h1_dists = return_mmd_distributions(\n",
    "    h0_paths, \n",
    "    h1_paths, \n",
    "    estimator,\n",
    "    n_atoms=n_atoms, \n",
    "    n_paths=n_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fac_crit_val = fac_h0_dists.sort()[0][int(n_atoms*(1-alpha))]\n",
    "\n",
    "plt.figure(figsize=golden_dimensions(4))\n",
    "n_bins = int(n_atoms/8)\n",
    "plt.title(f\"Expected Type II error: {100*expected_type2_error(fac_h1_dists, fac_crit_val):.2f}%, $\\phi(k) = (k/2)!$\", fontsize=\"small\")\n",
    "plt.hist(fac_h0_dists, bins=n_bins, color=\"dodgerblue\", alpha=0.5, label=\"$H_0$\", density=True)\n",
    "plt.hist(fac_h1_dists, bins=n_bins, color=\"tomato\"    , alpha=0.5, label=\"$H_1$\", density=True)\n",
    "plt.legend()\n",
    "make_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Again let's test to see everything is working \n",
    "# Note that it's unlikely the convergence will be \"perfect\" as we are approximating with the PDE method\n",
    "# and calculating explicitly with the truncated level-k method. Need larger dyadic order too\n",
    "\n",
    "scalings_ = np.arange(25, 101)\n",
    "n_runs    = 25\n",
    "true_mmds = torch.zeros((scalings_.shape[0], n_runs))\n",
    "\n",
    "for i, n_scaling in enumerate(tqdm(scalings_)):\n",
    "    for run in range(n_runs):\n",
    "        this_kernel     = PhiKernel(dyadic_order=0, n_scalings=n_scaling)\n",
    "        true_mmds[i, run] = this_kernel.compute_mmd(testX, testY)\n",
    "\n",
    "final_mmd_est = true_mmds[-1].mean()\n",
    "print(f\"True MMD (estimate, {scalings_[-1]} scalings): {final_mmd_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "\n",
    "estimate_mmd = mmd_est_k(testX, testY, max_k, phik=phik)\n",
    "\n",
    "print(f\"Estimate MMD: {estimate_mmd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mmd = true_mmds.mean(axis=1)\n",
    "sd = true_mmds.std(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=golden_dimensions(4))\n",
    "ax.fill_between(scalings_, mean_mmd, mean_mmd+sd, color=\"dodgerblue\", alpha=0.25, label=\"$\\pm 1 \\sigma$\")\n",
    "ax.plot(scalings_, mean_mmd, label=\"mmd_estimate\", color=\"dodgerblue\", alpha=0.5)\n",
    "ax.fill_between(scalings_, mean_mmd, mean_mmd-sd, color=\"dodgerblue\", alpha=0.25)\n",
    "ax.axhline(estimate_mmd, color=\"tomato\", alpha=0.5, label=f\"mmd_trunc_sig_{max_k}\")\n",
    "ax.legend(fontsize=\"small\")\n",
    "ax.set_xlabel(\"Number of sampled scalings\")\n",
    "ax.set_ylabel(\"Value of MMD\")\n",
    "ax.set_title(\"Number of scalings versus estimate of $\\phi$-MMD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Example plot \n",
    "_conts = np.array([M_k(testX, testY, _k, phik=phik) for _k in range(max_k)]).cumsum()\n",
    "\n",
    "plt.figure(figsize=golden_dimensions(4))\n",
    "\n",
    "plt.title(f\"Unbiased sample MMD vs level-$k$ estimate MMD convergence ($N={n_paths}$)\", fontsize=\"small\")\n",
    "n_bins = int(n_atoms/8)\n",
    "plt.axhline(true_mmds[-1].cpu(), linestyle=\"dashed\", color=\"tomato\", alpha=0.75, label=\"true_mmd_est\")\n",
    "plt.plot(np.arange(max_k), _conts, color=\"dodgerblue\", alpha=0.5, label=\"level_k_mmd_est\")\n",
    "plt.legend()\n",
    "make_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly some estimation error. It looks like quite a few scalings are needed with the signature kernel to get a true estimate of the corresponding $\\phi$-kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_k   = 6\n",
    "\n",
    "fac_h0_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "fac_h1_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "\n",
    "rand_ints = torch.randint(0, batch_size, size=(n_atoms, 3, n_paths))\n",
    "\n",
    "for i, ii in enumerate(tqdm(rand_ints)):\n",
    "    # Sample some paths\n",
    "    \n",
    "    x1, x2 = h0_paths[ii[0]], h0_paths[ii[1]]\n",
    "    y      = h1_paths[ii[-1]]\n",
    "    \n",
    "    for j, _k in enumerate(range(1, max_k + 1)):  # 0 order is not interesting\n",
    "        # Calculate level-k contribution\n",
    "        fac_h0_cont_dists[j, i] = M_k(x1, x2, _k, phik=phik)\n",
    "        fac_h1_cont_dists[j, i] = M_k(x1, y, _k, phik=phik) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# First plot side-by-side\n",
    "fig, ax = plt.subplots(1, max_k, figsize=(max_k*5, 5))\n",
    "\n",
    "n_bins = int(n_atoms/8)\n",
    "\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.hist(fac_h0_cont_dists[i], bins = n_bins, color=\"dodgerblue\", alpha=0.5, label=f\"h0\", density=True)\n",
    "    axi.hist(fac_h1_cont_dists[i], bins = n_bins, color=\"tomato\"    , alpha=0.5, label=f\"h1\", density=True)\n",
    "    make_grid(axis=axi)\n",
    "    axi.legend(fontsize=\"small\")\n",
    "    axi.set_title(f\"Level {i+1}\")\n",
    "fig.suptitle(\"Distribution of level contributions\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the positions of each histogram\n",
    "n_bins = int(n_atoms/8)\n",
    "\n",
    "x_positions = np.arange(max_k+1)[::-1]\n",
    "\n",
    "plot_dat        = np.zeros((2, max_k + 1, n_atoms))\n",
    "plot_dat[0, 0]  = fac_h0_dists\n",
    "plot_dat[1, 0]  = fac_h1_dists\n",
    "plot_dat[0, 1:] = fac_h0_cont_dists\n",
    "plot_dat[1, 1:] = fac_h1_cont_dists\n",
    "\n",
    "# Plot the histograms\n",
    "colors= [\"dodgerblue\", \"tomato\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for j, dat in enumerate(plot_dat):\n",
    "    \n",
    "    for i, dat_ in enumerate(dat):\n",
    "        hist, bin_edges = np.histogram(dat_, bins=n_bins, density=False)\n",
    "        bin_centers     = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        width = np.diff(bin_centers)[0]\n",
    "        \n",
    "        axes[j].bar(bin_centers, hist, zs=x_positions[i], zdir='y', alpha=0.8, width=width, color=colors[j])\n",
    "        axes[j].set_xlabel(\"$M_k$\")\n",
    "        \n",
    "        yticks      = x_positions + 1\n",
    "        yticklabels = [\"$D(P,Q)^2$\"] + [f\"level-{k}\" for k in range(1, max_k+1)]\n",
    "        \n",
    "        axes[j].set_yticks(yticks)\n",
    "        axes[j].set_yticklabels(yticklabels)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Contribution of signature terms, null and alternate hypotheses, $\\phi(k) = (k/2)!$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Other scaling options: \n",
    "\n",
    "#### A \"guesstimate\": $\\phi(k) = dt^{-k/2}$\n",
    "\n",
    "What seems to work (in practice at least) is a scaling more like $\\phi(k) = dt^{-k/2}$. We are working on a more rigourous statement to try and find the correct scaling.\n",
    "\n",
    "It also may/may not help to normalise the time channel: we do so here, but can be easily remidied. This is a \\emph{temporal based approach} that might leverage \\emph{annualizing} the stochastic process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def batch_path_lengths(paths):\n",
    "    \"\"\"\n",
    "    Computes the lengths of a batch of piecewise linear paths in R^d.\n",
    "    \n",
    "    Args:\n",
    "    paths (Tensor): a tensor of shape (N, l, d) representing N paths, each with l points in R^d.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: a tensor of shape (N,) representing the lengths of the paths.\n",
    "    \"\"\"\n",
    "    # Compute the differences between successive points on each path\n",
    "    diff = paths[:, 1:] - paths[:, :-1]\n",
    "    \n",
    "    # Compute the Euclidean norm of the differences\n",
    "    diff_norm = torch.norm(diff, dim=2)\n",
    "    \n",
    "    # Sum up the norms of the differences to get the total length of each path\n",
    "    lengths = torch.sum(diff_norm, dim=1)\n",
    "    \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dt = torch.diff(ts)[0]\n",
    "average_path_length  = torch.mean(batch_path_lengths(h0_paths))\n",
    "average_state_length = (average_path_length - 1)/state_size\n",
    "\n",
    "#_scale = torch.pow(dt, -0.5)\n",
    "_scale = 1\n",
    "\n",
    "static_kernel = sigkernel.RBFKernel(sigma=1e-2)\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel=static_kernel, dyadic_order=2)\n",
    "max_batch = 16\n",
    "estimator = lambda x, y: signature_kernel.compute_mmd(x, y, max_batch=max_batch)\n",
    "\n",
    "time_normalisation = True\n",
    "\n",
    "h0_scaled = _scale*h0_paths.clone()\n",
    "h1_scaled = _scale*h1_paths.clone()\n",
    "\n",
    "if time_normalisation:\n",
    "    h0_scaled[..., 0]  /= _scale*T\n",
    "    h1_scaled[...,  0] /= _scale*T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "scale_h0_dists, scale_h1_dists = return_mmd_distributions(\n",
    "    h0_scaled, \n",
    "    h1_scaled, \n",
    "    estimator, \n",
    "    n_atoms=n_atoms, \n",
    "    n_paths=n_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sc_crit_val = scale_h0_dists.sort()[0][int(n_atoms*(1-alpha))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=golden_dimensions(4))\n",
    "n_bins = int(n_atoms/8)\n",
    "fig.suptitle(f\"Expected Type II error: {100*expected_type2_error(scale_h1_dists, sc_crit_val):.2f}%, $\\phi(k) = {_scale:.2f}^k$\", fontsize=\"small\")\n",
    "ax.hist(scale_h0_dists/scale_h0_dists.max(), bins=n_bins, color=\"dodgerblue\", alpha=0.5, label=\"$H_0$\", density=True)\n",
    "ax.hist(scale_h1_dists/scale_h0_dists.max(), bins=n_bins, color=\"tomato\"    , alpha=0.5, label=\"$H_1$\", density=True)\n",
    "ax.legend()\n",
    "make_grid(axis=ax)\n",
    "# plt.savefig(\"workedexampledistsdtsq.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_k_sc = 10\n",
    "\n",
    "sc_testX = h0_scaled[:n_paths]\n",
    "sc_testY = h1_scaled[:n_paths]\n",
    "\n",
    "true_mmd_sc = signature_kernel.compute_mmd(sc_testX, sc_testY, max_batch=16)\n",
    "\n",
    "print(f\"True MMD: {true_mmd_sc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimate_mmd = mmd_est_k(sc_testX, sc_testY, max_k_sc)\n",
    "\n",
    "print(f\"Estimate MMD: {estimate_mmd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_conts = np.array([M_k(sc_testX, sc_testY, _k) for _k in range(max_k_sc)]).cumsum()\n",
    "\n",
    "plt.figure(figsize=golden_dimensions(4))\n",
    "#plt.title(f\"Unbiased sample MMD vs level-$k$ estimate MMD convergence ($N={n_paths}$)\", fontsize=\"small\")\n",
    "n_bins = int(n_atoms/8)\n",
    "plt.axhline(true_mmd_sc.cpu(), linestyle=\"dashed\", color=\"tomato\", alpha=0.75, label=r\"$\\mathcal{D}_{k_{\\mathrm{Sig}}}(\\mathbf{X}, \\mathbf{Y})^2$\")\n",
    "plt.plot(np.arange(max_k_sc), _conts, color=\"dodgerblue\", alpha=0.5, label=r\"$\\sum_{k> 0}^n M_k(\\mathbf{X}, \\mathbf{Y})$\")\n",
    "plt.legend()\n",
    "# make_grid()\n",
    "# plt.savefig(\"convergence_to_true_phik_dtsq.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence now happens much later: compared to the base case, at around $k=10$, compared to around $k=3$ in the unscaled case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_k   = 5\n",
    "\n",
    "sc_h0_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "sc_h1_cont_dists = torch.zeros((max_k, n_atoms))\n",
    "\n",
    "rand_ints = torch.randint(0, batch_size, size=(n_atoms, 3, n_paths))\n",
    "\n",
    "for i, ii in enumerate(tqdm(rand_ints)):\n",
    "    # Sample some paths\n",
    "    \n",
    "    x1, x2 = h0_scaled[ii[0]], h0_scaled[ii[1]]\n",
    "    y      = h1_scaled[ii[-1]]\n",
    "    \n",
    "    for j, _k in enumerate(range(1, max_k + 1)):  # 0 order is not interesting\n",
    "        # Calculate level-k contribution\n",
    "        sc_h0_cont_dists[j, i] = M_k(x1, x2, _k)\n",
    "        sc_h1_cont_dists[j, i] = M_k(x1, y, _k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# First plot side-by-side\n",
    "\n",
    "fig, ax = plt.subplots(1, max_k, figsize=(max_k*5, 5))\n",
    "\n",
    "n_bins = int(n_atoms/8)\n",
    "\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.hist(sc_h0_cont_dists[i], bins = n_bins, color=\"dodgerblue\", alpha=0.5, label=f\"h0\", density=True)\n",
    "    axi.hist(sc_h1_cont_dists[i], bins = n_bins, color=\"tomato\"    , alpha=0.5, label=f\"h1\", density=True)\n",
    "    make_grid(axis=axi)\n",
    "    axi.legend(fontsize=\"small\")\n",
    "    axi.set_title(f\"Level {i+1}\")\n",
    "fig.suptitle(\"Distribution of level contributions, scaled\");\n",
    "# plt.savefig(\"levelcontributionsdtsq.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the positions of each histogram\n",
    "n_bins = int(n_atoms/8)\n",
    "\n",
    "x_positions = np.arange(max_k+1)[::-1]\n",
    "\n",
    "plot_dat        = np.zeros((2, max_k + 1, n_atoms))\n",
    "plot_dat[0, 0]  = scale_h0_dists\n",
    "plot_dat[1, 0]  = scale_h1_dists\n",
    "plot_dat[0, 1:] = sc_h0_cont_dists\n",
    "plot_dat[1, 1:] = sc_h1_cont_dists\n",
    "\n",
    "# Plot the histograms\n",
    "colors= [\"dodgerblue\", \"tomato\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for j, dat in enumerate(plot_dat):\n",
    "    \n",
    "    for i, dat_ in enumerate(dat):\n",
    "        hist, bin_edges = np.histogram(dat_, bins=n_bins, density=False)\n",
    "        bin_centers     = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        width = np.diff(bin_centers)[0]\n",
    "        \n",
    "        axes[j].bar(bin_centers, hist, zs=x_positions[i], zdir='y', alpha=0.8, width=width, color=colors[j])\n",
    "        axes[j].set_xlabel(\"$M_k$\")\n",
    "        \n",
    "        yticks      = x_positions + 1\n",
    "        yticklabels = [\"$D(P,Q)^2$\"] + [f\"level-{k}\" for k in range(1, max_k+1)]\n",
    "        \n",
    "        axes[j].set_yticks(yticks)\n",
    "        axes[j].set_yticklabels(yticklabels)\n",
    "\n",
    "\n",
    "elev = 30 # Example elevation\n",
    "azim = -60 # Example azimuth\n",
    "\n",
    "axes[0].view_init(elev=elev, azim=azim)\n",
    "axes[1].view_init(elev=elev, azim=azim)\n",
    "\n",
    "fig.suptitle(\"Contribution of signature terms, null and alternate hypotheses, $\\phi(k) = dt^{-k/2}$\");\n",
    "# plt.savefig(\"levelcontributionhistsdtsq.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater contributions occur much later - as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative density of letters in level $k$\n",
    "\n",
    "Consider time-augmented paths $X: [a,b] \\to \\mathbb{R}$. Suppose you are interested in having data like the $k^{\\text{th}}$ moment be the most relavant in the calculation of the signature kernel MMD. \n",
    "\n",
    "##### Notation\n",
    "\n",
    "1. $\\mathcal{A}_d$: alphabet of letters $\\{1, 2, \\dots, d\\}$. \n",
    "2. $\\mathcal{W}(\\mathcal{A}_d)$: Space of words comprised of letters from $\\mathcal{A}_d$ \n",
    "3. $\\mathcal{W}^N(\\mathcal{A}_d) \\subset \\mathcal{W}(\\mathcal{A}_d)$: Space of words of length $N \\in \\mathbb{N}$\n",
    "4. $\\xi: \\mathcal{W}(\\mathcal{A}_d) \\times \\mathcal{A}_d \\to \\mathbb{N}$: function that returns the number of times the letter $i$ appears in the word $\\mathbf{w}$.\n",
    "\n",
    "Here we fix $d=2$ (time-augmented univariate process). \n",
    "\n",
    "<b>Hypothesis</b>: the level $k$ that contains the highest <i>relative density</i> of terms like $$S(X)_{[a,b]}^{I^k_N}, \\quad I^k_N \\in \\{\\xi(I_N, i) = k : I_N \\in \\mathcal{W}^N(\\mathcal{A}_2) \\}$$ is the level which should contribute maximally to the MMD. \n",
    "\n",
    "We define the following terms (for general $d \\in \\mathbb{N}$):\n",
    "\n",
    "1. The $k$-<i>incidence</i> of the letter $i$ at the level $N$ is given by $$\\iota^N(i, k) = \\sum_{w \\in \\mathcal{W}^N(\\mathcal{A}_d)} \\chi_{\\{\\xi(w, i) = k\\}}(w).$$ When $d=2$, $\\iota^N(i, k) = {N \\choose k}$ where $N$ is the length of the word $\\boldsymbol{w}$.\n",
    "\n",
    "2. The $k$-<i>density</i> of the letter $i$ at the level $N$ is given by $$\\delta^N(i, k) = \\frac{\\iota^N(i, k)}{|\\mathcal{W}^N(\\mathcal{A}_d)|}$$ where $|\\cdot|$ denotes the number of words that can be made with $N$ letters. In fact, it is given by $$|\\mathcal{W}^N(\\mathcal{A}_d)| = d^N.$$ When $d=2$, this reduces to $$\\delta^N(i, k) = \\frac{{N \\choose k}}{2^N} = \\frac{N!}{2^Nk!(N-k)!}.$$\n",
    "\n",
    "Some notes: \n",
    "\n",
    "1. The $k$-incidence is an increasing function in $N$ (obvious: for $d > 1$, all terms contributing to the $k$-incidence at level $N$ also contribute at level $N+1$ when integrated against channels not corresponding to $i$).\n",
    "\n",
    "2. <b>Hypothesis</b>: There exists a (not necessarily unique) maximal $N\\in \\mathbb{N}$ such that the $k$-density of a letter $i$ is maximised.\n",
    "\n",
    "3. <b>Question</b>: Does the function $N \\mapsto \\delta^N(i, k)$ have a maximum?\n",
    "\n",
    "##### Method\n",
    "\n",
    "We stick to the time-augmented univariate case here.\n",
    "\n",
    "1. Find the $N \\in \\mathbb{N}$ that maximises the $k$-density of the letter $2$.\n",
    "2. Find the scaling $\\lambda \\in \\mathbb{R}$ that makes the term like $L(\\lambda x)^N/N!$ the largest. \n",
    "3. Apply this scaling; check the results.\n",
    "\n",
    "This technique can be easily extended to a distribution scalings, representing emphasis weights on certain moments. One then needs to solve the appropriate Hamburger moment problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# First, let's check all the assumptions about n choose k and so on hold.\n",
    "def nchoosek(N, k):\n",
    "    return math.factorial(N)/(math.factorial(k)*math.factorial(N-k))\n",
    "\n",
    "def generate_words(N, d):\n",
    "    alphabet = torch.arange(d)  # Create the alphabet as a tensor from 0 to d-1\n",
    "    words = torch.zeros((d**N, N), dtype=torch.long)  # Initialize a tensor to store the generated words\n",
    "\n",
    "    for i in range(N):\n",
    "        repeat = d**(N-i-1)  # Number of times to repeat each letter\n",
    "        tile   = d**i        # Number of times to tile the repeated letters\n",
    "\n",
    "        letters = alphabet.repeat(repeat)  # Repeat each letter\n",
    "        letters = letters.unsqueeze(1).repeat(1, tile).view(-1)  # Tile the repeated letters\n",
    "\n",
    "        words[:, i] = letters  # Assign the letters to the corresponding column of the words tensor\n",
    "\n",
    "    return words\n",
    "    \n",
    "\n",
    "def k_incidence(N, d, k, i):\n",
    "    \"\"\"\n",
    "    Returns the number of words of length :param N: from the alphabet of :param d: letters \n",
    "    that contain the letter :param i: :param k: times\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    words = generate_words(N, d)\n",
    "    \n",
    "    for w in words:\n",
    "        letter_freq = sum(i == w)\n",
    "        res += letter_freq == k\n",
    "        \n",
    "    return res\n",
    "    \n",
    "def k_density(N, d, k, i):\n",
    "    return k_incidence(N, d, k, i)/torch.pow(d, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Example with the second moment\n",
    "\n",
    "d = torch.tensor(state_size + 1)  # Number of dimensions in asset price process\n",
    "i = 1                             # Stock price channel \n",
    "k = 2                             # Second moment \n",
    "max_N = 12                        # An example\n",
    "\n",
    "N_range = torch.arange(1, max_N + 1)\n",
    "d_range = torch.pow(d, N_range)\n",
    "\n",
    "k_incidences = torch.tensor([k_incidence(Ni, d, k, i) for Ni in N_range])\n",
    "k_densities  = k_incidences/d_range\n",
    "\n",
    "fig, ax = plt.subplots(figsize=golden_dimensions(4))\n",
    "ax.plot(k_incidences, color=\"dodgerblue\", label=\"$\\iota^N(k, 1)$\", alpha=0.5)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(k_densities, color=\"tomato\", label=\"$\\delta^N(k, 1)$\", alpha=0.5)\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine handles and labels\n",
    "handles = handles1 + handles2\n",
    "labels  = labels1 + labels2\n",
    "\n",
    "# Create the combined legend\n",
    "ax.legend(handles, labels, loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Here it appears the second/third level should matter the most \n",
    "# We will scale so that the maximum contribution comes from these values\n",
    "# We now plot the bounds function associated to the h0_paths \n",
    "\n",
    "average_path_length = torch.mean(batch_path_lengths(h0_paths))\n",
    "bounds = torch.tensor([torch.pow(average_path_length, k)/math.factorial(k) for k in N_range])\n",
    "plt.plot(bounds, color=\"dodgerblue\", alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
